{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CC - 2021.01.26 - Plots for the paper",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "neptune": {
      "notebookId": "f9d84060-100d-49d1-86bd-569d716ea7de"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf_C-d1uVGaG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7OGUjxNfkac",
        "outputId": "9ac04810-4301-4b08-8280-38cb2146b335"
      },
      "source": [
        "!pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZwXN2rp2D7k"
      },
      "source": [
        "from itertools import cycle\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import neptune\n",
        "import seaborn\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EueWvim-A8nV"
      },
      "source": [
        "def full_experiment_name(experiment):\n",
        "    params = experiment.get_parameters()\n",
        "    if 'size_after_pool' in params:\n",
        "        return f\"{experiment.name}_pool_size_{params['size_after_pool']}\"\n",
        "    else:\n",
        "        return experiment.name\n",
        "\n",
        "def download_artifacts(exp):\n",
        "    if os.path.isdir(\"output\"):\n",
        "        shutil.rmtree(\"output\")\n",
        "        \n",
        "    exp.download_artifacts()\n",
        "\n",
        "    with zipfile.ZipFile(\"output.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4H0b_LjjNGA"
      },
      "source": [
        "def mean_max(head_probs):\n",
        "    head_certainty, head_answer = head_probs.max(-1)\n",
        "    return head_answer, head_certainty\n",
        "\n",
        "def mean_entropy(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    head_answer = head_ensemble_probs.argmax(-1)\n",
        "    head_certainty = -torch.distributions.categorical.Categorical(head_ensemble_probs).entropy()\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n",
        "\n",
        "def mean_second_diff(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    (values, indices) = torch.sort(head_ensemble_probs, dim=-1, descending=True)\n",
        "    head_answer = indices[:, 0]\n",
        "    head_certainty = values[:, 0] - values[:, 1]\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n",
        "\n",
        "def count_agreement(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    head_answer = head_ensemble_probs.argmax(-1)\n",
        "    ensemble_answers = head_probs.argmax(-1)\n",
        "    head_certainty = torch.true_divide((ensemble_answers == head_answer.unsqueeze(1)).sum(dim=-1),\n",
        "                                       head_probs.size(dim=1))\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DbxiGE9kx72"
      },
      "source": [
        "def calibrate_heads(method, preds, labels, iters=5000, lr=1e-2 / 2, optimizer_class=torch.optim.SGD, params=None):\n",
        "    assert preds.size(2) == 1\n",
        "    calib_head_preds = []\n",
        "    calib_head_params = []\n",
        "    for i in range(preds.size(1)):\n",
        "        print(f'Calibrating head {i}')\n",
        "        head_preds = preds[:, i,...].squeeze()\n",
        "        calib_params = None if params is None else params[i]\n",
        "        calibrated_head_preds, calib_params = method(head_preds, labels, iters=iters, lr=lr, optimizer_class=optimizer_class, params=calib_params)\n",
        "        calib_head_preds.append(calibrated_head_preds)\n",
        "        calib_head_params.append(calib_params)\n",
        "    # unsqueeze so there is 1 net in the \"ensemble\"\n",
        "    return torch.stack(calib_head_preds, dim=1).unsqueeze(2), calib_head_params\n",
        "\n",
        "\n",
        "def temperature_scaling(preds, labels, iters, lr, optimizer_class, params=None):\n",
        "    correct_preds = (preds.argmax(-1) == labels).float().detach()\n",
        "    \n",
        "    if params is not None:\n",
        "        t = params\n",
        "    else:\n",
        "        t = torch.tensor([1.], requires_grad=True)\n",
        "        optimizer = optimizer_class([t], lr=lr)\n",
        "        for idx in range(iters):\n",
        "            temped_l_probs = torch.log_softmax(preds.detach() / t, dim=-1)\n",
        "            nll = F.nll_loss(temped_l_probs, labels)\n",
        "            # max_probs = torch.softmax(preds.detach() * t, dim=-1).max(-1)[0]\n",
        "            # ll = torch.log(max_probs + 1e-8) * correct_preds + torch.log(1 - max_probs + 1e-8) * (1 - correct_preds)\n",
        "            # ll = -(max_probs - correct_preds).square()\n",
        "            # nll = -ll.mean()\n",
        "            optimizer.zero_grad()\n",
        "            nll.backward()\n",
        "            optimizer.step()\n",
        "            if idx % 100 == 0:\n",
        "                print(f'(temperature_scaling) nll: {nll.item()}')\n",
        "                pass\n",
        "    return torch.softmax(preds * t, dim=-1).cpu().detach(), t\n",
        "\n",
        "\n",
        "def vector_scaling(preds, labels, iters, lr, optimizer_class, params=None):\n",
        "    correct_preds = (preds.argmax(-1) == labels).float().detach()\n",
        "    \n",
        "    if params is not None:\n",
        "        w, b = params\n",
        "    else:\n",
        "        w = torch.ones(preds.size(-1), requires_grad=True)\n",
        "        b = torch.zeros(preds.size(-1), requires_grad=True)\n",
        "        optimizer = optimizer_class([w, b], lr=lr)\n",
        "        for idx in range(iters):\n",
        "            temped_l_probs = torch.log_softmax(w * preds.detach() + b, dim=-1)\n",
        "            nll = F.nll_loss(temped_l_probs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            nll.backward()\n",
        "            optimizer.step()\n",
        "            if idx % 100 == 0:\n",
        "                print(f'(vector_scaling) nll: {nll.item()}')\n",
        "                pass\n",
        "    return torch.softmax(w * preds.detach() + b, dim=-1).cpu().detach(), (w, b)\n",
        "\n",
        "\n",
        "def matrix_scaling(preds, labels, iters, lr, optimizer_class, params=None):\n",
        "    correct_preds = (preds.argmax(-1) == labels).float().detach()\n",
        "    \n",
        "    if params is not None:\n",
        "        w, b = params\n",
        "    else:\n",
        "        w = torch.diag(torch.ones(preds.size(-1)))\n",
        "        w.requires_grad = True\n",
        "        b = torch.zeros(preds.size(-1), requires_grad=True)\n",
        "        optimizer = optimizer_class([w, b], lr=lr)\n",
        "        for idx in range(iters):\n",
        "            temped_l_probs = torch.log_softmax(preds.detach() @ w + b, dim=-1)\n",
        "            nll = F.nll_loss(temped_l_probs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            nll.backward()\n",
        "            optimizer.step()\n",
        "            if idx % 100 == 0:\n",
        "                print(f'(matrix_scaling) nll: {nll.item()}')\n",
        "                pass\n",
        "    return torch.softmax(preds.detach() @ w + b, dim=-1).cpu().detach(), (w, b)\n",
        "\n",
        "\n",
        "def dirichlet_calibration(probs, labels, iters, lr, optimizer_class, params=None):\n",
        "    ln_probs = probs.log()\n",
        "    correct_preds = (probs.argmax(-1) == labels).float().detach()\n",
        "    \n",
        "    if params is not None:\n",
        "        w, b = params\n",
        "    else:\n",
        "        w = torch.diag(torch.ones(probs.size(-1)))\n",
        "        w.requires_grad = True\n",
        "        b = torch.zeros(probs.size(-1), requires_grad=True)\n",
        "        optimizer = optimizer_class([w, b], lr=lr)\n",
        "        for idx in range(iters):\n",
        "            temped_l_probs = torch.log_softmax(probs.log().detach() @ w + b, dim=-1)\n",
        "            nll = F.nll_loss(temped_l_probs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            nll.backward()\n",
        "            optimizer.step()\n",
        "            if idx % 100 == 0:\n",
        "                print(f'(dirichlet_calibration) nll: {nll.item()}')\n",
        "                pass\n",
        "    return torch.softmax(probs.log().detach() @ w + b, dim=-1).cpu().detach(), (w, b)\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "ids_to_calibrate = []\n",
        "# ids_to_calibrate = ['CON1-676', 'CON1-677', 'CON1-676_ada_weighting', 'CON1-677_r']\n",
        "ids_to_add = []\n",
        "for exp_id in ids_to_calibrate:\n",
        "    if experiment_preds_softmaxed[exp_id] == True:\n",
        "        methods = [dirichlet_calibration]\n",
        "    else:\n",
        "        # methods = [temperature_scaling, vector_scaling, matrix_scaling]\n",
        "        methods = [temperature_scaling, matrix_scaling]\n",
        "    for calibration_method in methods:\n",
        "        new_id = f'{exp_id}_{calibration_method.__name__}'\n",
        "        \n",
        "        experiment_names[new_id] = f'{experiment_names[exp_id]}_{calibration_method.__name__}'\n",
        "        experiment_cls_weights[new_id] = experiment_cls_weights[exp_id]\n",
        "\n",
        "        train_logits = experiment_train_logits[exp_id]\n",
        "        train_last_logits = experiment_train_last_logits[exp_id]\n",
        "        train_labels = experiment_train_labels[exp_id]\n",
        "        print(f'calibrating {experiment_names[exp_id]}')\n",
        "        experiment_train_logits[new_id], calib_params = calibrate_heads(calibration_method, train_logits, train_labels)\n",
        "        experiment_train_last_logits[new_id] = torch.softmax(train_last_logits, dim=-1)\n",
        "        experiment_train_labels[new_id] = train_labels\n",
        "\n",
        "        test_logits = experiment_test_logits[exp_id]\n",
        "        test_last_logits = experiment_test_last_logits[exp_id]\n",
        "        test_labels = experiment_test_labels[exp_id]\n",
        "        # note that training calib params are used here\n",
        "        experiment_test_logits[new_id], _ = calibrate_heads(calibration_method, test_logits, test_labels, params=calib_params)\n",
        "        experiment_test_last_logits[new_id] = torch.softmax(test_last_logits, dim=-1)\n",
        "        experiment_test_labels[new_id] = test_labels\n",
        "\n",
        "        experiment_total_params[new_id] = experiment_total_params[exp_id]\n",
        "        experiment_total_ops[new_id] = experiment_total_ops[exp_id]\n",
        "\n",
        "        experiment_preds_softmaxed[new_id] = True\n",
        "        \n",
        "        if not torch.isnan(experiment_train_logits[new_id]).any():\n",
        "            ids_to_add.append(new_id)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhDYJTQG1_zy"
      },
      "source": [
        "def plot_roc_curves(data, head_i):\n",
        "    current_palette = cycle(seaborn.color_palette('tab10'))\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    for name, auc_scores, curve_data, accs in data:\n",
        "        current_color = next(current_palette)\n",
        "        plt.plot(curve_data[head_i][0], curve_data[head_i][1], \n",
        "                 label=f'{name} CA-AUC={auc_scores[head_i]:0.2f} ACC={accs[head_i]:0.2f}', color=current_color)\n",
        "        \n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.title(f'head {head_i}')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_aucs(data):\n",
        "    current_palette = cycle(seaborn.color_palette(\"tab10\"))\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "    ax1.set_ylabel(\"Correct-answer AUC\")\n",
        "    ax1.set_xlabel(\"Layer\")\n",
        "    # ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    ax2.set_xlabel(\"Layer\")\n",
        "    for name, auc_scores, _, accs in data:\n",
        "        current_color = next(current_palette)\n",
        "        ax1.scatter(np.arange(len(auc_scores)) + 1, auc_scores, label=name, color=current_color, marker='x')    \n",
        "        ax2.scatter(np.arange(len(auc_scores)) + 1, accs, color=current_color, marker='+')\n",
        "    # ax1.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# auc_data = []\n",
        "# for exp_id in experiment_subset_ids:\n",
        "#     test_logits = experiment_test_logits[exp_id]\n",
        "#     test_last_logits = experiment_test_last_logits[exp_id]\n",
        "#     test_labels = experiment_test_labels[exp_id]\n",
        "#     auc_scores, roc_curves = uncertainty_auroc_check(test_logits, test_last_logits, test_labels, softmax=not experiment_preds_softmaxed[exp_id])\n",
        "#     last_acc, ens_accs, _ = check_head_pred_acc(test_logits, test_last_logits, test_labels, softmax=not experiment_preds_softmaxed[exp_id])\n",
        "#     accs = ens_accs + [last_acc]\n",
        "#     auc_data.append((experiment_names[exp_id], auc_scores, roc_curves, accs))\n",
        "\n",
        "\n",
        "\n",
        "def uncertainty_auroc_check(logits, last_logits, labels, unc_method=mean_max, softmax=True):\n",
        "    head_certainty_responses = [[] for _ in range(logits.size(1) + 1)]\n",
        "    head_correct_responses = [[] for _ in range(logits.size(1) + 1)]\n",
        "\n",
        "    probs = logits.softmax(-1) if softmax else logits\n",
        "    last_probs = last_logits.softmax(-1) if softmax else last_logits\n",
        "\n",
        "    answered = torch.tensor([-1 for _ in range(len(last_logits))])\n",
        "    \n",
        "    for head_i in range(logits.size(1)):\n",
        "        head_probs = probs[:,head_i, :, :]\n",
        "        head_answer, head_certainty, head_ensemble_probs = unc_method(head_probs)\n",
        "        head_correct = head_answer == labels\n",
        "        head_certainty_responses[head_i].append(head_certainty.cpu().detach().numpy())\n",
        "        head_correct_responses[head_i].append(head_correct.cpu().detach().numpy())\n",
        "\n",
        "    last_answer, last_certainty, last_ensemble_probs = unc_method(last_probs.unsqueeze(1))\n",
        "    last_correct = last_answer == labels\n",
        "    head_certainty_responses[logits.size(1)].append(last_certainty.cpu().detach().numpy())\n",
        "    head_correct_responses[logits.size(1)].append(last_correct.cpu().detach().numpy())\n",
        "\n",
        "    head_certainty_responses = [\n",
        "        np.concatenate(certainty_responses)\n",
        "        for certainty_responses in head_certainty_responses\n",
        "    ]\n",
        "    head_correct_responses = [\n",
        "        np.concatenate(correct_responses)\n",
        "        for correct_responses in head_correct_responses\n",
        "    ]\n",
        "\n",
        "    roc_curves = []\n",
        "    auc_scores = []\n",
        "    for certainty_responses, correct_responses in zip(\n",
        "            head_certainty_responses, head_correct_responses):\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(correct_responses,\n",
        "                                                 certainty_responses)\n",
        "        auc = metrics.auc(fpr, tpr)\n",
        "        roc_curves.append((fpr, tpr, thresholds))\n",
        "        auc_scores.append(auc)\n",
        "\n",
        "    return auc_scores, roc_curves\n",
        "\n",
        "def mean_max(head_probs):\n",
        "    head_certainty, head_answer = head_probs.max(-1)\n",
        "    return head_answer, head_certainty\n",
        "\n",
        "def mean_entropy(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    head_answer = head_ensemble_probs.argmax(-1)\n",
        "    head_certainty = -torch.distributions.categorical.Categorical(head_ensemble_probs).entropy()\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n",
        "\n",
        "def mean_second_diff(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    (values, indices) = torch.sort(head_ensemble_probs, dim=-1, descending=True)\n",
        "    head_answer = indices[:, 0]\n",
        "    head_certainty = values[:, 0] - values[:, 1]\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n",
        "\n",
        "def count_agreement(head_probs):\n",
        "    head_ensemble_probs = head_probs.mean(dim=1)\n",
        "    head_answer = head_ensemble_probs.argmax(-1)\n",
        "    ensemble_answers = head_probs.argmax(-1)\n",
        "    head_certainty = torch.true_divide((ensemble_answers == head_answer.unsqueeze(1)).sum(dim=-1),\n",
        "                                       head_probs.size(dim=1))\n",
        "    return head_answer, head_certainty, head_ensemble_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCYBJdf32D8w",
        "scrolled": false
      },
      "source": [
        "def confidence_ece(probs, labels, n_bins = 20):\n",
        "    num_examples = labels.size(0)\n",
        "    max_probs, max_inds = probs.max(-1)\n",
        "    bins = torch.linspace(0., 1., steps=n_bins + 1)\n",
        "    conf_ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        bin_start = bins[i]\n",
        "        bin_end = bins[i + 1]\n",
        "        examples_in_bin = (bin_start <= max_probs) * (max_probs < bin_end)\n",
        "        bin_size = examples_in_bin.sum().item()\n",
        "        if bin_size > 0:\n",
        "            bin_acc = (max_inds[examples_in_bin] == labels[examples_in_bin]).sum().item() / bin_size\n",
        "            bin_avg_confidence = max_probs[examples_in_bin].sum().item() / bin_size\n",
        "            conf_ece += bin_size * abs(bin_acc - bin_avg_confidence)\n",
        "    conf_ece /= num_examples\n",
        "    return conf_ece\n",
        "\n",
        "\n",
        "def classwise_ece(probs, labels, n_bins = 20):\n",
        "    num_examples = probs.size(0)\n",
        "    num_classes = probs.size(-1)\n",
        "    bins = torch.linspace(0., 1., steps=n_bins + 1)\n",
        "    class_ece = 0.0\n",
        "    for k in range(num_classes):\n",
        "        for i in range(n_bins):\n",
        "            bin_start = bins[i]\n",
        "            bin_end = bins[i + 1]\n",
        "            examples_in_bin = (bin_start <= probs[:, k]) * (probs[:, k] < bin_end)\n",
        "            bin_size = examples_in_bin.sum().item()\n",
        "            if bin_size > 0:\n",
        "                bin_class_prop = (labels[examples_in_bin] == k).sum().item() / bin_size\n",
        "                bin_avg_prob = probs[examples_in_bin][:, k].sum().item() / bin_size\n",
        "                class_ece += bin_size * abs(bin_class_prop - bin_avg_prob)\n",
        "    class_ece /= num_classes * num_examples\n",
        "    return class_ece\n",
        "\n",
        "def calibration_histogram(preds, labels, title):\n",
        "    # preds shape [N, C]\n",
        "    # labels shape [C]\n",
        "    \n",
        "    # probs = torch.softmax(preds, -1)\n",
        "    probs = preds\n",
        "    max_probs, max_inds = probs.max(1)\n",
        "    \n",
        "    bin_accs = []\n",
        "    bin_examples = []\n",
        "    bins_num = 20\n",
        "    for bin_idx in range(bins_num):\n",
        "        bin_start = bin_idx / bins_num\n",
        "        bin_end = bin_start + 1 / bins_num\n",
        "        \n",
        "        examples_in_bin = (bin_start < max_probs) * (max_probs < bin_end)\n",
        "        bin_examples = [examples_in_bin.sum()]\n",
        "        \n",
        "        bin_preds = preds[examples_in_bin]\n",
        "        bin_probs = probs[examples_in_bin]\n",
        "        bin_labels = labels[examples_in_bin]\n",
        "        \n",
        "        if len(bin_preds > 0):\n",
        "            bin_acc = (bin_preds.argmax(1) == bin_labels).float().mean()\n",
        "        else:\n",
        "            bin_acc = 0.\n",
        "        \n",
        "        bin_accs += [bin_acc]\n",
        "    bin_examples = np.array(bin_examples)\n",
        "        \n",
        "    x = np.arange(bins_num) / bins_num\n",
        "    error = np.abs(np.array(bin_accs) - np.arange(bins_num) / bins_num) * bin_examples\n",
        "    error = error.sum() / bin_examples.sum()\n",
        "    plt.title(title)\n",
        "    plt.bar(range(bins_num), bin_accs)\n",
        "    plt.plot(range(bins_num), np.arange(bins_num) / bins_num)\n",
        "    plt.scatter(range(bins_num), np.arange(bins_num) / bins_num, zorder=100)\n",
        "    plt.xticks(np.linspace(0, bins_num, num=5), np.linspace(0, 1, num=5))\n",
        "    \n",
        "    plt.show()\n",
        "    print(f\"Error {title}: {error}\")\n",
        "\n",
        "def get_calibrated_preds(preds, labels):\n",
        "    all_calibrated_preds = []\n",
        "    for idx in range(7):\n",
        "        calibration_histogram(preds[:, idx, :], labels, \"before calibration\")\n",
        "        calibrated_preds = calibrate_dataset(preds[:, idx, :], labels)\n",
        "        calibration_histogram(calibrated_preds, labels, \"after calibration\")\n",
        "\n",
        "        all_calibrated_preds.append(calibrated_preds)\n",
        "    all_calibrated_preds = torch.stack(all_calibrated_preds, dim=1)\n",
        "    return all_calibrated_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55LHwzhcox3"
      },
      "source": [
        "def get_eces(logits, labels, bins=20, softmax=True):\n",
        "    probs = torch.softmax(logits, dim=-1) if softmax else logits\n",
        "    num_heads = logits.size(1)\n",
        "    conf_eces = []\n",
        "    class_eces = []\n",
        "\n",
        "    for head_i in range(num_heads):\n",
        "        head_probs = probs[:,head_i, :]\n",
        "        conf_eces.append(confidence_ece(head_probs, labels))\n",
        "        class_eces.append(classwise_ece(head_probs, labels))\n",
        "\n",
        "    return np.array(conf_eces), np.array(class_eces)\n",
        "\n",
        "def get_ensemble_eces(exp, dataset=\"test\", unc_method=mean_max, bins=20):\n",
        "    probs = torch.softmax(exp[f'{dataset}_logits'], dim=-1)\n",
        "    num_heads = probs.size(1)\n",
        "\n",
        "    conf_eces = []\n",
        "    class_eces = []\n",
        "\n",
        "    for head_i in range(num_heads):\n",
        "        head_probs = probs[:,head_i, :]\n",
        "        conf_eces.append(confidence_ece(head_probs, exp[f'{dataset}_labels']))\n",
        "        class_eces.append(classwise_ece(head_probs, exp[f'{dataset}_labels']))\n",
        "\n",
        "    return np.array(conf_eces), np.array(class_eces)\n",
        "\n",
        "\n",
        "def plot_eces(data):\n",
        "    current_palette = cycle(seaborn.color_palette(\"tab10\"))\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(16, 16))\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "    ax1.set_ylabel(\"Confidence ECE\")\n",
        "    ax1.set_xlabel(\"Layer\")\n",
        "    # ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"Classwise ECE\")\n",
        "    ax2.set_xlabel(\"Layer\")\n",
        "    for name, conf_eces, class_eces in data:\n",
        "        current_color = next(current_palette)\n",
        "        marker_type = \"o\" if \"scaling\" in name else \"x\" \n",
        "        ax1.scatter(np.arange(len(conf_eces)) + 1, conf_eces, label=name, color=current_color,\n",
        "                    marker=marker_type)    \n",
        "        ax2.scatter(np.arange(len(class_eces)) + 1, class_eces, color=current_color, marker=marker_type)\n",
        "    ax1.legend()\n",
        "    ax2.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# experiment = experiment_subset[-1]\n",
        "# test_logits = experiment_test_logits[exp_id]\n",
        "# test_last_logits = experiment_test_last_logits[exp_id]\n",
        "# test_labels = experiment_test_labels[exp_id]\n",
        "# ece_data = []\n",
        "# conf_eces, class_eces = get_ensemble_eces(test_logits, test_labels)\n",
        "# ece_data.append((full_experiment_name(experiment), conf_eces, class_eces))\n",
        "# for net_i in range(test_logits.size(-2)):\n",
        "#     conf_eces, class_eces = get_eces(test_logits[:, :, net_i, :], test_labels)\n",
        "#     ece_data.append((f'{full_experiment_name(experiment)}_net_{net_i}', conf_eces, class_eces))\n",
        "# plot_eces(ece_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsCICC8D2D71"
      },
      "source": [
        "def get_head_accs(logits, labels):\n",
        "    head_accs = (logits.argmax(-1) == labels.unsqueeze(1)).float().mean(0).numpy()\n",
        "    return head_accs\n",
        "\n",
        "def head_improvement_matrix(exp, dataset=\"test\", softmax=True):\n",
        "    preds = exp[f'{dataset}_logits'].softmax(-1)\n",
        "    labels = exp[f'{dataset}_labels']\n",
        "    \n",
        "    improved_counts = torch.zeros(\n",
        "        preds.size(1),\n",
        "        preds.size(1),\n",
        "        dtype=int)\n",
        "    total_counts = torch.zeros(\n",
        "        preds.size(1),\n",
        "        preds.size(1),\n",
        "        dtype=int)\n",
        "\n",
        "    corrects = preds.argmax(-1) == labels.unsqueeze(-1)\n",
        "    for idx in range(preds.size(1)):\n",
        "        failed_indices = corrects[:, idx] == False\n",
        "        improved_counts[idx, :] += corrects[failed_indices].sum(dim=0)\n",
        "        total_counts[idx, :] += failed_indices.sum(dim=0)\n",
        "\n",
        "    head_improvement = (improved_counts.float() /\n",
        "                        total_counts.float()).detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(30, 30))\n",
        "    plt.ylabel(\"How many misclassified by head...\")\n",
        "    plt.xlabel(\"...are answered correctly by head\")\n",
        "    plt.xticks(list(range(preds.size(1))))\n",
        "    plt.yticks(list(range(preds.size(1))))\n",
        "    plt.imshow(head_improvement, vmin=0., vmax=1., cmap=plt.get_cmap('viridis'))\n",
        "    for row_idx, row in enumerate(head_improvement):\n",
        "        for col_idx, val in enumerate(row):\n",
        "            val_str = f\"{val * 100:.2f}%\"\n",
        "            plt.text(col_idx,\n",
        "                     row_idx,\n",
        "                     val_str,\n",
        "                     horizontalalignment=\"center\",\n",
        "                     c=\"white\")\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    return head_improvement, fig\n",
        "\n",
        "\n",
        "\n",
        "def get_bounds(logits, unc_method=mean_max, softmax=True):\n",
        "    num_heads = logits.size(1)\n",
        "    probs = logits.softmax(-1) if softmax else logits\n",
        "    ensemble_probs = probs.mean(dim=2)\n",
        "    min_value = math.inf\n",
        "    max_value = -math.inf\n",
        "    for head_i in range(num_heads):\n",
        "        head_probs = probs[:,head_i, :]\n",
        "        _, head_certainty = unc_method(head_probs)\n",
        "        min_value = min(min_value, head_certainty.min().item())\n",
        "        max_value = max(max_value, head_certainty.max().item())\n",
        "    return min_value, max_value\n",
        "\n",
        "def ensemble_predictor(exp, threshold=1., dataset=\"test\", unc_method=mean_max):\n",
        "    probs = exp[f'{dataset}_logits'].softmax(-1)\n",
        "    answered = torch.tensor([-1] * len(probs))\n",
        "    ensemble_pred = torch.tensor([-1] * len(probs))\n",
        "    \n",
        "    for head_i in range(exp['num_heads']):\n",
        "        head_probs = probs[:, head_i, :]\n",
        "        head_answer, head_certainty = unc_method(head_probs)\n",
        "        agreement = (head_certainty > threshold) * (answered < 0)\n",
        "        answered[agreement] = head_i\n",
        "        ensemble_pred[agreement] = head_answer[agreement]\n",
        "    # unanswered samples get the original (last head) answer\n",
        "    ensemble_pred[answered < 0] = probs[:, head_i, :].argmax(-1)[answered < 0]\n",
        "    answered[answered < 0] = head_i\n",
        "\n",
        "    head_ops = exp['total_ops'].expand(len(probs), -1)\n",
        "    avg_ops = head_ops.gather(1, answered.unsqueeze(1)).mean().item()\n",
        "    acc = (ensemble_pred == exp[f'{dataset}_labels']).float().mean().item()\n",
        "    return acc, avg_ops\n",
        "    \n",
        "def ensemble_check(exp, x_linspace, dataset=\"test\", unc_method=mean_max):\n",
        "    accs = []\n",
        "    avg_ops = []\n",
        "    for thresh in x_linspace:\n",
        "        acc, avg_op = ensemble_predictor(\n",
        "            exp, thresh, dataset=dataset, unc_method=unc_method)\n",
        "        accs.append(acc)\n",
        "        avg_ops.append(avg_op)\n",
        "    return accs, avg_ops\n",
        "\n",
        "def patience_check(exp, dataset=\"test\"):\n",
        "    accs = []\n",
        "    avg_ops = []\n",
        "    for patience_val in range(1, exp['num_heads'] + 1):\n",
        "        acc, avg_op = patience_predictor(\n",
        "            exp, patience_val, dataset=dataset)\n",
        "        accs.append(acc)\n",
        "        avg_ops.append(avg_op)\n",
        "    return accs, avg_ops\n",
        "\n",
        "def patience_predictor(exp, patience_thresh, dataset=\"test\"):\n",
        "    preds = exp[f'{dataset}_logits'].argmax(-1)\n",
        "\n",
        "    answered = torch.tensor([-1] * len(preds))\n",
        "    ensemble_pred = torch.tensor([-1] * len(preds))\n",
        "    prev_pred = torch.tensor([-1] * len(preds))\n",
        "    patience = torch.tensor([0] * len(preds))\n",
        "    \n",
        "    for head_i in range(exp['num_heads']):\n",
        "        head_preds = preds[:, head_i]\n",
        "        patience = torch.where(\n",
        "            head_preds == prev_pred,\n",
        "            patience + 1,\n",
        "            1\n",
        "        )\n",
        "        agreement = (patience >= patience_thresh) * (answered < 0)\n",
        "        answered[agreement] = head_i\n",
        "        ensemble_pred[agreement] = head_preds[agreement]\n",
        "        prev_pred = head_preds.clone()\n",
        "    # unanswered samples get the original (last head) answer\n",
        "    ensemble_pred[answered < 0] = head_preds[answered < 0]\n",
        "    answered[answered < 0] = head_i\n",
        "\n",
        "    head_ops = exp['total_ops'].expand(len(preds), -1)\n",
        "    avg_ops = head_ops.gather(1, answered.unsqueeze(1)).mean().item()\n",
        "    acc = (ensemble_pred == exp[f'{dataset}_labels']).float().mean().item()\n",
        "    return acc, avg_ops\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JhaHCMnRfg6"
      },
      "source": [
        "## Improvability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxBWabt9An1s"
      },
      "source": [
        "FONT_SIZE = 22\n",
        "FIVE_THIRTY_EIGHT = {\n",
        "        \"SDN\": \"#30a2da\",\n",
        "        \"PABEE\": \"#fc4f30\",\n",
        "        \"Zero Time Waste\": \"#e5ae38\",\n",
        "        \"Stacking\": \"#6d904f\",\n",
        "        \"Ensembling\": \"#810f7c\",\n",
        "}\n",
        "\n",
        "def improvability(exp, dataset=\"test\", show_plot=False):\n",
        "    improvability = [0.]\n",
        "    correctness = (exp[f'{dataset}_logits'].argmax(-1) == exp[f'{dataset}_labels'].unsqueeze(1)).numpy()\n",
        "    for head_idx in range(1, correctness.shape[1]):\n",
        "        wrong_ids = (correctness[:, head_idx] == False) > 0\n",
        "        # print((correctness[wrong_ids, :head_idx].sum(1) > 0).mean())\n",
        "        improv = (correctness[wrong_ids, :head_idx].sum(1) > 0).mean() # - (1 - 0.9 ** head_idx)\n",
        "        improvability += [improv] \n",
        "    improvability = np.array(improvability)\n",
        "\n",
        "    return improvability[1:]\n",
        "\n",
        "def correctness(exp, dataset=\"test\"):\n",
        "    improvability = [0.]\n",
        "    correctness = (exp[f'{dataset}_logits'].argmax(-1) == exp[f'{dataset}_labels'].unsqueeze(1)).numpy()\n",
        "    for head_idx in range(1, correctness.shape[1]):\n",
        "        wrong_ids = (correctness[:, head_idx] == True)\n",
        "        # print((correctness[wrong_ids, -1].astype(float) > 0))\n",
        "        improv = (correctness[wrong_ids, -2].astype(float) > 0).mean() # - (1 - 0.9 ** head_idx)\n",
        "        improvability += [improv]\n",
        "    improvability = np.array(improvability)\n",
        "    return improvability[1:]\n",
        "\n",
        "\n",
        "\n",
        "def plot_improvability(exps, title=\"\"):\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    color_scheme = FIVE_THIRTY_EIGHT\n",
        "    for exp_id, exp in exps.items():\n",
        "        if exp_id == \"Base Network\" or exp_id == \"SDN+Stacking\":\n",
        "            continue\n",
        "        improv = improvability(exp)\n",
        "        plt.plot(\n",
        "            np.arange(len(improv)) + 2, improv, marker=\"o\", markersize=20, linewidth=3.,\n",
        "                    label=exp_id, c=color_scheme[exp_id])\n",
        "        # plt.scatter(np.arange(len(improv)) + 1, improv, s=60, label=exp_id)\n",
        "    num_heads = exp['num_heads']\n",
        "    num_classes = exp['test_logits'].shape[-1]\n",
        "    \n",
        "\n",
        "    base = (1 / num_classes)\n",
        "    random_improv = (1 - base ** np.arange(num_heads))[1:]\n",
        "    print(random_improv)\n",
        "    # plt.plot(np.arange(len(random_improv)) + 1, random_improv, '--', marker=\"o\", markersize=7.75, color=\"gray\", label=\"Random Baseline\")\n",
        "    plt.xlabel(\"IC\", fontsize=FONT_SIZE)\n",
        "    plt.ylabel(\"Hindsight Improvability\", fontsize=FONT_SIZE)\n",
        "    plt.title(title + \"\\n(lower is better)\", fontdict={'fontsize': FONT_SIZE + 1})\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
        "    for tick in plt.gca().xaxis.get_major_ticks():\n",
        "        tick.label.set_fontsize(FONT_SIZE - 4) \n",
        "    for tick in plt.gca().yaxis.get_major_ticks():\n",
        "        tick.label.set_fontsize(FONT_SIZE - 4)\n",
        "\n",
        "\n",
        "    plt.legend(prop={'size': FONT_SIZE})\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6C3-ZS7aCkP"
      },
      "source": [
        "## Time-Acc Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG026dgIbD-d"
      },
      "source": [
        "import matplotlib as mpl\n",
        "from matplotlib.legend_handler import HandlerLine2D, HandlerTuple\n",
        "\n",
        "\n",
        "def draw_time_acc_plot(data, ax, first=False, title=None):\n",
        "    \n",
        "    current_palette = FIVE_THIRTY_EIGHT\n",
        "    # current_symbols = [\"^\", \"s\", ]\n",
        "\n",
        "    baseline_name, _, _, baseline_acc, baseline_ops  = data[0]\n",
        "    baseline_ops = baseline_ops.item()\n",
        "\n",
        "\n",
        "    for name, acc, avg_ops, head_accs, head_ops in data[1:]:\n",
        "        print(\"OPS\", avg_ops, baseline_ops)\n",
        "        avg_ops = np.array(avg_ops)\n",
        "        current_color = current_palette[name]\n",
        "        ax.plot(\n",
        "            avg_ops, acc, label=name,\n",
        "            ls=\"-\",\n",
        "            linewidth=3.5,\n",
        "            color=current_color)\n",
        "        # plt.scatter(avg_ops, acc, color=current_color)\n",
        "        if name == \"PABEE\":\n",
        "            continue\n",
        "        ax.scatter(head_ops, head_accs, s=90, marker='o', color=current_color,\n",
        "                    edgecolors=\"black\", linewidths=0., zorder=3)\n",
        "\n",
        "    \n",
        "    ax.scatter(\n",
        "        baseline_ops, baseline_acc, s=250, marker='X',\n",
        "        color=\"black\", label=baseline_name, zorder=3, linewidths=0.)\n",
        "\n",
        "    ax.set_xlim(right=1.1 * baseline_ops)\n",
        "    ax.set_ylabel('Accuracy', fontsize=FONT_SIZE)\n",
        "    ax.set_xlabel('Inference Time', fontsize=FONT_SIZE)\n",
        "    ax.set_title(title, fontdict={'fontsize': FONT_SIZE + 1})\n",
        "\n",
        "    ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(baseline_ops / 4))\n",
        "    ax.xaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=baseline_ops))\n",
        "\n",
        "    for tick in ax.xaxis.get_major_ticks():\n",
        "        tick.label.set_fontsize(FONT_SIZE - 4) \n",
        "    for tick in ax.yaxis.get_major_ticks():\n",
        "        tick.label.set_fontsize(FONT_SIZE - 4) \n",
        "\n",
        "    \n",
        "    if first:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        circles = []\n",
        "        for name, _, _, _, _ in data[1:]:\n",
        "            if name == \"PABEE\":\n",
        "                continue\n",
        "            circles += [mpl.lines.Line2D([], [], color=FIVE_THIRTY_EIGHT[name], marker='o', linestyle='None',\n",
        "                                markersize=10, label='IC')]\n",
        "\n",
        "        handles += [tuple(circles)]\n",
        "        labels += [\"IC\"]\n",
        "        ax.legend(handles=handles, labels=labels, prop={'size': FONT_SIZE}, handler_map={tuple: HandlerTuple(ndivide=None)})\n",
        "    \n",
        "\n",
        "def plot_time_acc(experiments, patience_id=None, dev=False):\n",
        "    dataset = \"test\"\n",
        "    print(\"=\" * 15, dataset.upper(), \"=\" * 15)\n",
        "    time_acc_data = []\n",
        "    for exp_id, exp in experiments.items():\n",
        "        print(f'Total params for experiment {exp_id}: {exp[\"total_params\"]}')\n",
        "        head_accs = get_head_accs(exp[f'{dataset}_logits'], exp[f'{dataset}_labels'])\n",
        "        unc_method = mean_max\n",
        "        min_value, max_value = get_bounds(exp[f'{dataset}_logits'], unc_method=unc_method)\n",
        "        x_linspace = np.concatenate([\n",
        "            np.linspace(min_value, 0.95, num=150),\n",
        "            np.linspace(0.95, 1., num=150)])\n",
        "        x_linspace = np.linspace(min_value, 1., num=200)\n",
        "        if dev:\n",
        "            x_linspace = np.linspace(min_value, 1., num=10)\n",
        "        accs, avg_ops = ensemble_check(exp, x_linspace, dataset=dataset, unc_method=unc_method)\n",
        "        time_acc_data.append((f'{exp_id}', accs, avg_ops, head_accs, exp['total_ops']))\n",
        "    \n",
        "    if patience_id is not None:\n",
        "        exp = experiments[patience_id]\n",
        "        head_accs = get_head_accs(exp[f'{dataset}_logits'], exp[f'{dataset}_labels'])\n",
        "        accs, avg_ops = patience_check(exp, dataset=dataset)\n",
        "\n",
        "        patience_data = ('PABEE', accs, avg_ops, head_accs, exp['total_ops'])\n",
        "        time_acc_data = time_acc_data[:-1] + [patience_data] + [time_acc_data[-1]]\n",
        "    return time_acc_data\n",
        "\n",
        "def draw_table(experiments, baseline_id, dataset=\"test\", patience_id=None):\n",
        "    baseline_exp = experiments[baseline_id]\n",
        "    baseline_ops = baseline_exp['total_ops'][0]\n",
        "    baseline_acc = (\n",
        "        baseline_exp[f'{dataset}_logits'].squeeze().argmax(-1) == baseline_exp[f'{dataset}_labels']).float().mean()\n",
        "\n",
        "    print(f\"Baseline: {baseline_acc * 100:.1f}%\")\n",
        "\n",
        "    for exp_id, exp in experiments.items():\n",
        "        if exp_id == baseline_id:\n",
        "            continue\n",
        "        unc_method = mean_max\n",
        "        min_value, max_value = get_bounds(exp[f'{dataset}_logits'], unc_method=unc_method)\n",
        "        x_linspace = np.linspace(min_value, 1., num=1000)\n",
        "        accs, avg_ops = ensemble_check(exp, x_linspace, dataset=dataset, unc_method=unc_method)\n",
        "        avg_ops = torch.tensor(avg_ops)\n",
        "        accs = torch.tensor(accs)\n",
        "\n",
        "        print(exp[\"name\"])\n",
        "        print(\"& \", end=\"\")\n",
        "        for thresh in [0.25, 0.5, 0.75, 1.]:\n",
        "            ops_thresh = thresh * baseline_ops\n",
        "            last_acc = accs[avg_ops < ops_thresh][-1]\n",
        "            print(f\"{last_acc * 100:.1f}\", end=\" & \")\n",
        "        print(f\"{accs.max() * 100:.1f} \")\n",
        "    \n",
        "    if patience_id is not None:\n",
        "        exp = experiments[patience_id]\n",
        "        accs, avg_ops = patience_check(exp, dataset=dataset)\n",
        "        avg_ops = torch.tensor(avg_ops)\n",
        "        accs = torch.tensor(accs)\n",
        "\n",
        "        print(\"patience\")\n",
        "        for thresh in [0.25, 0.5, 0.75, 1.]:\n",
        "            ops_thresh = thresh * baseline_ops\n",
        "            last_acc = accs[avg_ops < ops_thresh][-1]\n",
        "            print(f\"{last_acc * 100:.1f}\", end=\" & \")\n",
        "        print(f\"{accs.max() * 100:.1f} & \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUBJEReJD-gT"
      },
      "source": [
        "def prepare_regular_experiment(exp_id):\n",
        "    exp = project.get_experiments(exp_id)[0]\n",
        "    exp_dict = {}\n",
        "    exp_dict['name'] = full_experiment_name(exp)\n",
        "    download_artifacts(exp)\n",
        "    \n",
        "    test_logits = torch.load('output/test_logits', map_location=\"cpu\")\n",
        "    # train_logits = torch.load('output/train_logits', map_location=\"cpu\")\n",
        "    \n",
        "    if len(test_logits.shape) > 2:  # last_logits non-None\n",
        "        test_last_logits = torch.load('output/test_last_logits', map_location=\"cpu\")\n",
        "        test_logits = torch.cat([\n",
        "            test_logits.squeeze(),\n",
        "            test_last_logits.unsqueeze(1),\n",
        "        ], 1)\n",
        "    \n",
        "        # train_last_logits = torch.load('output/train_last_logits', map_location=\"cpu\")\n",
        "        # train_logits = torch.cat([\n",
        "        #     train_logits.squeeze(),\n",
        "        #     train_last_logits.unsqueeze(1),\n",
        "        # ], 1)\n",
        "    else:\n",
        "        # train_logits = train_logits.unsqueeze(1)\n",
        "        test_logits = test_logits.unsqueeze(1)\n",
        "    \n",
        "    # exp_dict['train_logits'] = train_logits\n",
        "    # exp_dict['train_labels'] = torch.load('output/train_labels', map_location='cpu')\n",
        "    exp_dict['test_logits'] = test_logits\n",
        "    exp_dict['test_labels'] = torch.load('output/test_labels', map_location='cpu')\n",
        "\n",
        "\n",
        "    \n",
        "    total_params = torch.load('output/total_params', map_location=\"cpu\")\n",
        "    total_ops = torch.load('output/total_ops', map_location=\"cpu\")\n",
        "    \n",
        "    if isinstance(total_params, dict):\n",
        "        total_params = torch.tensor([total_params[n] for n in range(max(total_params.keys()) + 1)])\n",
        "        total_ops = torch.tensor([total_ops[n] for n in range(max(total_ops.keys()) + 1)])\n",
        "    \n",
        "    print(exp_id, total_ops.shape)\n",
        "    exp_dict['total_params'] = total_params\n",
        "    exp_dict['total_ops'] = total_ops\n",
        "    exp_dict['num_heads'] = test_logits.shape[1]\n",
        "    \n",
        "    return exp_dict\n",
        "\n",
        "\n",
        "\n",
        "def prepare_ensb_experiment(tag):\n",
        "    exps = project.get_experiments(tag)[0]\n",
        "    exp_dict = {}\n",
        "    exp_dict['name'] = full_experiment_name(exp)\n",
        "    download_artifacts(exp)\n",
        "    \n",
        "    test_logits = torch.load('output/test_logits', map_location=\"cpu\")\n",
        "    # train_logits = torch.load('output/train_logits', map_location=\"cpu\")\n",
        "    \n",
        "    if len(test_logits.shape) > 2:  # last_logits non-None\n",
        "        test_last_logits = torch.load('output/test_last_logits', map_location=\"cpu\")\n",
        "        test_logits = torch.cat([\n",
        "            test_logits.squeeze(),\n",
        "            test_last_logits.unsqueeze(1),\n",
        "        ], 1)\n",
        "    \n",
        "        # train_last_logits = torch.load('output/train_last_logits', map_location=\"cpu\")\n",
        "        # train_logits = torch.cat([\n",
        "        #     train_logits.squeeze(),\n",
        "        #     train_last_logits.unsqueeze(1),\n",
        "        # ], 1)\n",
        "    else:\n",
        "        # train_logits = train_logits.unsqueeze(1)\n",
        "        test_logits = test_logits.unsqueeze(1)\n",
        "    \n",
        "    # exp_dict['train_logits'] = train_logits\n",
        "    # exp_dict['train_labels'] = torch.load('output/train_labels', map_location='cpu')\n",
        "    exp_dict['test_logits'] = test_logits\n",
        "    exp_dict['test_labels'] = torch.load('output/test_labels', map_location='cpu')\n",
        "    \n",
        "    total_params = torch.load('output/total_params', map_location=\"cpu\")\n",
        "    total_ops = torch.load('output/total_ops', map_location=\"cpu\")\n",
        "    \n",
        "    if isinstance(total_params, dict):\n",
        "        total_params = torch.tensor([total_params[n] for n in range(max(total_params.keys()) + 1)])\n",
        "        total_ops = torch.tensor([total_ops[n] for n in range(max(total_ops.keys()) + 1)])\n",
        "    \n",
        "    print(total_params, total_ops)\n",
        "    exp_dict['total_params'] = total_params\n",
        "    exp_dict['total_ops'] = total_ops\n",
        "    exp_dict['num_heads'] = test_logits.shape[1]\n",
        "    \n",
        "    return exp_dict\n",
        "\n",
        "def prepare_run_ensb_experiment(tag):\n",
        "    exps = project.get_experiments(tag=tag)\n",
        "\n",
        "    exp_dict = {}\n",
        "    exp_dict['name'] = tag\n",
        "\n",
        "    child_tags = list(tag for tag in exps[0].get_tags() if \"child_of_\" in tag)\n",
        "    assert len(child_tags) == 1\n",
        "    parent_tag = child_tags[0].replace(\"child_of_\", \"\")\n",
        "    parent_exps = project.get_experiments(tag=parent_tag)\n",
        "    assert len(parent_exps) == 1, parent_exps\n",
        "\n",
        "    download_artifacts(parent_exps[0])\n",
        "\n",
        "    total_params = torch.load(\"output/total_params\", map_location=\"cpu\")\n",
        "    total_ops = torch.load(\"output/total_ops\", map_location=\"cpu\")\n",
        "    \n",
        "    total_params = torch.tensor([total_params[n] for n in range(max(total_params.keys()) + 1)])\n",
        "    total_ops = torch.tensor([total_ops[n] for n in range(max(total_ops.keys()) + 1)])\n",
        "\n",
        "    head_dict = {}\n",
        "    for exp in exps:\n",
        "        key = int(exp.get_parameters()['head_idx'])\n",
        "        head_dict[key] = exp\n",
        "\n",
        "    # train_logits = []\n",
        "    test_logits = []\n",
        "\n",
        "    for key in range(max(head_dict.keys()) + 1):\n",
        "        exp = head_dict[key]\n",
        "        download_artifacts(exp)\n",
        "        # train_logits += [torch.load(\"output/train_logits\", map_location=\"cpu\")]\n",
        "        test_logits += [torch.load(\"output/test_logits\", map_location=\"cpu\")]\n",
        "\n",
        "    # exp_dict['train_logits'] = torch.stack(train_logits, 1)\n",
        "    exp_dict['test_logits'] = torch.stack(test_logits, 1)\n",
        "\n",
        "    num_classes = exp_dict['test_logits'].shape[-1]\n",
        "\n",
        "    # for idx in range(0, len(total_ops)):\n",
        "    #     total_ops[idx] += (((idx + 1) * num_classes * 2 - 1) * num_classes) / 1e9\n",
        "\n",
        "\n",
        "    new_ops = torch.zeros_like(total_ops)\n",
        "    new_ops[0] = (num_classes + num_classes) / 1e9\n",
        "    for idx in range(1, len(total_ops)):\n",
        "        mul_ops = (idx + 1) * num_classes\n",
        "        add_ops = idx * num_classes\n",
        "        bias_ops = num_classes \n",
        "\n",
        "        new_ops[idx] = new_ops[idx - 1] + (mul_ops + add_ops + num_classes) / 1e9\n",
        "        # total_ops[idx] += (((idx + 1) * num_classes * 2 - 1) * num_classes) / 1e9\n",
        "    \n",
        "    print(new_ops, total_ops)\n",
        "    exp_dict['total_params'] = total_params\n",
        "    exp_dict['total_ops'] = total_ops + new_ops\n",
        "\n",
        "    print(tag, total_ops.shape)\n",
        "    exp_dict['num_heads'] = len(exps)\n",
        "    \n",
        "    # exp_dict['train_labels'] = torch.load(\"output/train_labels\", map_location=\"cpu\")\n",
        "    exp_dict['test_labels'] = torch.load(\"output/test_labels\", map_location=\"cpu\")\n",
        "\n",
        "    return exp_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac_24Q0bBDKv"
      },
      "source": [
        "# Time-Acc Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_rLfM6oIgCO"
      },
      "source": [
        "project = neptune.init('TODO fill me', api_token='=')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDJ2F8uCzMkn",
        "outputId": "ec70aeef-40d7-4ed2-932a-ff45f7957379"
      },
      "source": [
        "cifar10_experiments = {}\n",
        "\n",
        "cifar10_experiments[\"Base Network\"] = prepare_regular_experiment(\"CON1-2505\")\n",
        "cifar10_experiments[\"SDN\"] = prepare_regular_experiment(\"CON1-2508\")\n",
        "# experiments[\"SDN+Stacking\"] = prepare_regular_experiment(\"CON1-2507\")\n",
        "cifar10_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar10_mobilenet_running_ensb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euqp3jraFmIx",
        "outputId": "3dd4245e-ce61-419f-c7ee-d01ff4335ada"
      },
      "source": [
        "cifar100_experiments = {}\n",
        "\n",
        "cifar100_experiments[\"Base Network\"] = prepare_regular_experiment(\"CON1-2948\")\n",
        "cifar100_experiments[\"SDN\"] = prepare_regular_experiment(\"CON1-2963\")\n",
        "# experiments[\"SDN+Stacking\"] = prepare_regular_experiment(\"CON1-2964\")\n",
        "cifar100_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar100_vgg16bn_running_ensb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3dprDhaBDKz",
        "outputId": "fc13735a-005e-4461-edb0-9d4e058a6581"
      },
      "source": [
        "tinyimagenet_experiments = {}\n",
        "\n",
        "tinyimagenet_experiments[\"Base Network\"] = prepare_regular_experiment(\"CON1-3223\")\n",
        "tinyimagenet_experiments[\"SDN\"] = prepare_regular_experiment(\"CON1-3354\")\n",
        "# experiments[\"SDN+Stacking\"] = prepare_regular_experiment(\"CON1-3355\")\n",
        "tinyimagenet_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210123_tinyimagenet_resnet56_running_ensb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "yAmYMFRFBDK0",
        "outputId": "70f6ed19-0a82-4dc4-f4b6-3029704fc84c"
      },
      "source": [
        "DEV = False\n",
        "fig, axes = plt.subplots(1, 1, figsize=(15, 9))\n",
        "axes = [axes]\n",
        "# [1, 2]?\n",
        "seaborn.set_style('whitegrid')\n",
        "# plt.style.use('fivethirtyeight')\n",
        "\n",
        "# time_acc_data = plot_time_acc(cifar10_experiments, patience_id='SDN', dev=DEV)\n",
        "# draw_time_acc_plot(time_acc_data, ax=axes[0], first=True, title=\"MobileNet - CIFAR-10\")\n",
        "\n",
        "# time_acc_data = plot_time_acc(cifar100_experiments, patience_id='SDN', dev=DEV)\n",
        "# draw_time_acc_plot(time_acc_data, ax=axes[1], first=False, title=\"VGG16 - CIFAR-100\")\n",
        "# \n",
        "time_acc_data = plot_time_acc(tinyimagenet_experiments, patience_id='SDN', dev=DEV)\n",
        "draw_time_acc_plot(time_acc_data, ax=axes[0], first=True, title=\"ResNet56 - Tiny ImageNet\")\n",
        "axes[0].set_xlabel('Inference Time', fontsize=FONT_SIZE)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "WoUev-xYZqj3",
        "outputId": "4b5e67c3-ded7-4dd0-c0b3-713c645edfcd"
      },
      "source": [
        "DEV = False\n",
        "fig, axes = plt.subplots(1, 2, figsize=(25, 10))\n",
        "seaborn.set_style('whitegrid')\n",
        "plt.tight_layout()\n",
        "\n",
        "# time_acc_data = plot_time_acc(cifar10_experiments, patience_id='SDN', dev=False)\n",
        "# draw_time_acc_plot(time_acc_data, ax=axes[0], first=True, title=\"MobileNet - CIFAR-10\")\n",
        "\n",
        "time_acc_data = plot_time_acc(cifar100_experiments, patience_id='SDN', dev=DEV)\n",
        "draw_time_acc_plot(time_acc_data, ax=axes[0], first=True, title=\"VGG16 on CIFAR-100\")\n",
        "\n",
        "time_acc_data = plot_time_acc(tinyimagenet_experiments, patience_id='SDN', dev=DEV)\n",
        "draw_time_acc_plot(time_acc_data, ax=axes[1], first=False, title=\"ResNet56 on TinyImagenet\")\n",
        "\n",
        "axes[1].set_ylabel('', fontsize=FONT_SIZE)\n",
        "\n",
        "\n",
        "plt.show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R128sg7fbFgE"
      },
      "source": [
        "fig.tight_layout()\n",
        "plt.show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDavZXEvBDK0",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef28b2a5-5d6f-4666-b0a9-44148a5ad4d8"
      },
      "source": [
        "for exp_id, exp in experiments.items():\n",
        "    head_improvement, fig = head_improvement_matrix(exp, dataset=\"test\")\n",
        "    plt.title(exp['name'])\n",
        "    plt.show(fig)\n",
        "    \n",
        "plot_improvability(experiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TtvBbiob52j"
      },
      "source": [
        "# Improvability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd7Qx37mrM-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d67c463-e17d-45c6-e3b2-25b60ae8ab13"
      },
      "source": [
        "c100_mobile_experiments = {}\n",
        "\n",
        "c100_mobile_experiments['Base Network'] = prepare_regular_experiment('CON1-2506')\n",
        "c100_mobile_experiments['SDN'] = prepare_regular_experiment('CON1-2509')\n",
        "# c100_mobile_experiments['SDN+Stacking'] = prepare_regular_experiment('CON1-2510')\n",
        "c100_mobile_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar100_mobilenet_running_ensb')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "iBDySl1j2_K8",
        "outputId": "43c034d5-050a-4cac-d212-c80484301012"
      },
      "source": [
        "plot_improvability(c100_mobile_experiments, title=\"Hindsight Improvability for MobileNet on CIFAR-100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xWJNQv4b7sG"
      },
      "source": [
        "# Ablations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmCPKn5JzPwc",
        "outputId": "26065ed6-9628-4bcb-90c6-2d9451cbf987"
      },
      "source": [
        "c100_vgg_experiments = {}\n",
        "\n",
        "c100_vgg_experiments['Base Network'] = prepare_regular_experiment('CON1-2948')\n",
        "c100_vgg_experiments['SDN'] = prepare_regular_experiment('CON1-2963')\n",
        "c100_vgg_experiments['Stacking'] = prepare_regular_experiment('CON1-2964')\n",
        "c100_vgg_experiments[\"Ensembling\"] = prepare_run_ensb_experiment('20210116_cifar100_vgg16bn_running_ensb_baseline')\n",
        "c100_vgg_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar100_vgg16bn_running_ensb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv2WVogg1_Le",
        "outputId": "a97d4f38-289a-4acc-b0f2-c739c092c80b"
      },
      "source": [
        "c100_resnet_experiments = {}\n",
        "\n",
        "c100_resnet_experiments['Base Network'] = prepare_regular_experiment('CON1-2544')\n",
        "c100_resnet_experiments['SDN'] = prepare_regular_experiment('CON1-2563')\n",
        "c100_resnet_experiments['Stacking'] = prepare_regular_experiment('CON1-2564')\n",
        "c100_resnet_experiments[\"Ensembling\"] = prepare_run_ensb_experiment('20210116_cifar100_resnet56_running_ensb_baseline')\n",
        "c100_resnet_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar100_resnet56_running_ensb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSKtHs270lAL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "276f97bc-369a-472d-8f0a-81bec3ca0a5e"
      },
      "source": [
        "ids_to_include = ['CON1-2544', 'CON1-2563', 'CON1-2564']\n",
        "# tags_to_include = ['20210109_first_exp_running_ensb_test', '20210109_first_exp_running_ensb_train']\n",
        "tags_to_include = ['20210116_cifar100_resnet56_running_ensb']\n",
        "\n",
        "c100_mobile_experiments = {}\n",
        "\n",
        "c10_mobile_experiments['Base Network'] = prepare_regular_experiment('CON1-2505')\n",
        "c10_mobile_experiments['SDN'] = prepare_regular_experiment('CON1-2508')\n",
        "c10_mobile_experiments['SDN+Stacking'] = prepare_regular_experiment('CON1-2507')\n",
        "c10_mobile_experiments[\"Zero Time Waste\"] = prepare_run_ensb_experiment('20210116_cifar10_mobilenet_running_ensb')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PzCcR2epz-fv",
        "outputId": "ea8b7b87-048a-4cd7-d529-437c3df1635a"
      },
      "source": [
        "DEV = False\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 18))\n",
        "# axes = [axes]\n",
        "seaborn.set_style('whitegrid')\n",
        "\n",
        "time_acc_data = plot_time_acc(c100_resnet_experiments, patience_id=None, dev=DEV)\n",
        "draw_time_acc_plot(time_acc_data, ax=axes[0], first=True, title=\"ResNet56 - CIFAR-100\")\n",
        "# axes[0].set_xlabel('GigaOps', fontsize=FONT_SIZE)\n",
        "\n",
        "time_acc_data = plot_time_acc(c100_vgg_experiments, patience_id=None, dev=DEV)\n",
        "draw_time_acc_plot(time_acc_data, ax=axes[1], first=False, title=\"VGG - CIFAR-100\")\n",
        "axes[1].set_xlabel('Inference Time', fontsize=FONT_SIZE)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRkYOeAV1ME"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}